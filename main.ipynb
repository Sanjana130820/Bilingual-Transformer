{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGuFztpaxoXQ"
   },
   "source": [
    "**Assignment 3**\n",
    "\n",
    "**Name: Sai Naga Sanjana Chippada**\n",
    "\n",
    "**ID: 017546776**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAHc7vDn40Ve"
   },
   "source": [
    "**English to French**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnpdWGg100zM",
    "outputId": "4a8b8e72-f5ab-460c-bb4d-000b5ebec111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.0\n",
      "  Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting torchtext==0.15.1\n",
      "  Downloading torchtext-0.15.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0) (3.1.5)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.0)\n",
      "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (2.32.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (1.26.4)\n",
      "Collecting torchdata==0.6.0 (from torchtext==0.15.1)\n",
      "  Downloading torchdata-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (892 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (75.1.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.45.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.3.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0) (3.31.4)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.0)\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
      "Downloading torch-2.0.0-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m839.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchtext-0.15.1-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchdata-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu124\n",
      "    Uninstalling torch-2.5.1+cu124:\n",
      "      Successfully uninstalled torch-2.5.1+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\n",
      "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0 torchtext-0.15.1 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.0 torchtext==0.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHn4mM-M01Pt",
    "outputId": "faba9c44-6651-47c4-d80b-27be74e2dd7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.0) (2.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.0) (2.32.3)\n",
      "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.0) (2.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchdata==0.6.0) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (75.1.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (0.45.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (3.31.4)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (18.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.6.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.6.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.6.0) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0->torchdata==0.6.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0->torchdata==0.6.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tClcV6hL04oq"
   },
   "outputs": [],
   "source": [
    "!pip install portalocker>=2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "tiYmybKx07Mj",
    "outputId": "fa9b8e73-bc69-4230-8cb7-04f1f51166e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
      "albumentations 2.0.4 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
      "chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
      "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\n",
      "pymc 5.20.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
      "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
      "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
      "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
      "bigframes 1.36.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
      "blosc2 3.0.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
      "langchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.23.5 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
      "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
      "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
      "treescope 0.1.8 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
      "scikit-image 0.25.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.23.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "643fa8a2372f4ff6b4adf90f5fc92f5f",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lh30WkAkXBLa",
    "outputId": "b4655e5f-ff8b-444f-cdf3-dc0a6e56b888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.23.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2025.1.31)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_sm\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U6xChLM11lxU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E23AsiepFayJ"
   },
   "outputs": [],
   "source": [
    "# Load spaCy tokenizers for English and French\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CpW7BMNgG-eO",
    "outputId": "948dd2de-7da2-4699-b0ce-1b866d50fe93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R5XNZRgmITnK"
   },
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_english(text):\n",
    "    return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
    "def tokenize_french(text):\n",
    "    return [token.text.lower() for token in spacy_fr.tokenizer(text)]\n",
    "\n",
    "english_text_file = \"english.txt\"\n",
    "french_text_file = \"french.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AD3Dhz9mI9H9"
   },
   "outputs": [],
   "source": [
    "with open(english_text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    english_sentences = f.read().splitlines()\n",
    "\n",
    "with open(french_text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    french_sentences = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7l6N6onW7GRM"
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "def yield_tokens(sentences, tokenizer):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab_en = build_vocab_from_iterator(yield_tokens(english_sentences, tokenize_english), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "vocab_fr = build_vocab_from_iterator(yield_tokens(french_sentences, tokenize_french), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])\n",
    "vocab_fr.set_default_index(vocab_fr[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "urr55Wvd7cAc"
   },
   "outputs": [],
   "source": [
    "# Convert sentences to tensor\n",
    "def numericalize(sentence, vocab, tokenizer):\n",
    "    return [vocab[\"<bos>\"]] + [vocab[token] for token in tokenizer(sentence)] + [vocab[\"<eos>\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hS3Kxd-uJTXm"
   },
   "outputs": [],
   "source": [
    "# Custom dataset class for translation\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src_tensor = torch.tensor(numericalize(self.src_sentences[index], self.src_vocab, self.src_tokenizer))\n",
    "        tgt_tensor = torch.tensor(numericalize(self.tgt_sentences[index], self.tgt_vocab, self.tgt_tokenizer))\n",
    "        return src_tensor, tgt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8gl2c4_wJaye"
   },
   "outputs": [],
   "source": [
    "# Collate function for padding sequences\n",
    "def collate_batch_translation(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=vocab_en[\"<pad>\"], batch_first=True)  # Use English vocab for source padding\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_fr[\"<pad>\"], batch_first=True)  # Use French vocab for target padding\n",
    "    return src_batch.to(device), tgt_batch.to(device)  # Move tensors to the specified device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3wmqvv7wOnQi"
   },
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "BATCH_SIZE = 32\n",
    "translation_dataset = TranslationDataset(english_sentences, french_sentences, vocab_en, vocab_fr, tokenize_english, tokenize_french)\n",
    "translation_loader = DataLoader(translation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RxXJzPccPSnV"
   },
   "outputs": [],
   "source": [
    "# Transformer Model Components - Positional Encoding\n",
    "class PositionalEncoding(nn.Module):  # No fixed sequence length\n",
    "    def __init__(self, model_dim, dropout_prob=0.1, max_tokens=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.encoding_matrix = torch.zeros(max_tokens, model_dim)\n",
    "        token_positions = torch.arange(0, max_tokens, dtype=torch.float).unsqueeze(1)\n",
    "        scale_factor = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
    "        self.encoding_matrix[:, 0::2] = torch.sin(token_positions * scale_factor)\n",
    "        self.encoding_matrix[:, 1::2] = torch.cos(token_positions * scale_factor)\n",
    "        self.encoding_matrix = self.encoding_matrix.unsqueeze(0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor = input_tensor + self.encoding_matrix[:, :input_tensor.size(1)].to(input_tensor.device)  # Align device and size\n",
    "        return self.dropout(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aVBcSKWzP2-A"
   },
   "outputs": [],
   "source": [
    "# Token Embedding\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_capacity, embed_dim):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embed_layer = nn.Embedding(vocab_capacity, embed_dim)\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        return self.embed_layer(input_tokens.long()) * math.sqrt(self.embed_layer.embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iCE0hpH-QFLp"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, model_dim=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, model_dim)\n",
    "        self.output_embedding = nn.Embedding(output_vocab_size, model_dim)\n",
    "        self.position_encoder = PositionalEncoding(model_dim, dropout)\n",
    "\n",
    "        # Pass correct argument names to nn.Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Linear(model_dim, output_vocab_size)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set device\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weight_range = 0.1\n",
    "        self.input_embedding.weight.data.uniform_(-weight_range, weight_range)\n",
    "        self.output_embedding.weight.data.uniform_(-weight_range, weight_range)\n",
    "        self.final_layer.bias.data.zero_()\n",
    "        self.final_layer.weight.data.uniform_(-weight_range, weight_range)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones((size, size), device=self.device)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def create_mask(self, src_seq, tgt_seq):\n",
    "        src_len = src_seq.shape[1]\n",
    "        tgt_len = tgt_seq.shape[1]\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_len).to(self.device)  # Ensure mask is on the correct device\n",
    "        src_mask = torch.zeros((src_len, src_len), device=self.device).type(torch.bool)\n",
    "\n",
    "        src_pad_mask = (src_seq == vocab_en[\"<pad>\"])  # src padding mask\n",
    "        tgt_pad_mask = (tgt_seq == vocab_fr[\"<pad>\"])  # tgt padding mask\n",
    "        return src_mask, tgt_mask, src_pad_mask, tgt_pad_mask\n",
    "\n",
    "    def forward(self, src_seq, tgt_seq):\n",
    "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = self.create_mask(src_seq, tgt_seq)\n",
    "\n",
    "        # Ensure src_seq and tgt_seq are on the correct device\n",
    "        src_embeddings = self.position_encoder(self.input_embedding(src_seq) * math.sqrt(self.model_dim)).to(self.device)\n",
    "        tgt_embeddings = self.position_encoder(self.output_embedding(tgt_seq) * math.sqrt(self.model_dim)).to(self.device)\n",
    "\n",
    "        transformer_out = self.transformer(src_embeddings, tgt_embeddings, src_mask=src_mask, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "\n",
    "        return self.final_layer(transformer_out)\n",
    "\n",
    "    def encode(self, src_seq):\n",
    "        src_embeddings = self.position_encoder(self.input_embedding(src_seq) * math.sqrt(self.model_dim)).to(self.device)\n",
    "        src_mask = torch.zeros((src_seq.shape[1], src_seq.shape[1]), device=self.device).type(torch.bool)\n",
    "        return self.transformer.encoder(src_embeddings, src_mask)\n",
    "\n",
    "    def decode(self, tgt_seq, encoded_memory):\n",
    "        tgt_embeddings = self.position_encoder(self.output_embedding(tgt_seq) * math.sqrt(self.model_dim)).to(self.device)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq.shape[1]).to(self.device)\n",
    "        return self.transformer.decoder(tgt_embeddings, encoded_memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mN1xHc4HQion",
    "outputId": "85e1a87a-23c0-4009-8ce8-e91843899b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (input_embedding): Embedding(13757, 256)\n",
      "  (output_embedding): Embedding(23814, 256)\n",
      "  (position_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=23814, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model parameters\n",
    "INPUT_DIM = len(vocab_en)\n",
    "OUTPUT_DIM = len(vocab_fr)\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Instantiate the Transformer model with correct parameter names\n",
    "model = TransformerModel(\n",
    "    INPUT_DIM,\n",
    "    OUTPUT_DIM,\n",
    "    model_dim=EMB_SIZE,\n",
    "    num_heads=NHEAD,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dim_feedforward=FFN_HID_DIM,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Print the model architecture to verify the setup\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l1GnLu0aSgC9"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_fr[\"<pad>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1S7BTxB4VBN8"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, translation_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for src, tgt in translation_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)  # Ensure data is moved to the correct device\n",
    "        optimizer.zero_grad()\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        output = model(src, tgt_input)  # Teacher forcing, pass source and target (except last token)\n",
    "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))  # Compare with target (except first token)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(translation_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IweBn59XVTfD"
   },
   "outputs": [],
   "source": [
    "def train(model, translation_loader, criterion, optimizer, epochs=3):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train_epoch(model, translation_loader, criterion, optimizer)\n",
    "        val_loss = evaluate(model, translation_loader, criterion)\n",
    "        print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Epoch time = {(time.time() - epoch_start_time):.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kHP7sBnRVW1o"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, translation_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in translation_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)  # Ensure data is on the correct device\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            output = model(src, tgt_input)\n",
    "            loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(translation_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eafijXYvVY2U"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src_tensor, src_vocab, tgt_vocab, max_len=50):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Encode the source sentence\n",
    "    with torch.no_grad():\n",
    "        memory = model.encode(src_tensor)\n",
    "\n",
    "    # Initialize the target sentence with <bos>\n",
    "    tgt_indices = [tgt_vocab[\"<bos>\"]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(model.device)  # (1, seq_len)\n",
    "\n",
    "        # Decode using the current sequence\n",
    "        with torch.no_grad():\n",
    "            output = model.decode(tgt_tensor, memory)\n",
    "            logits = model.final_layer(output[:, -1, :])  # Get logits for the last token\n",
    "\n",
    "            # Get the highest probability token\n",
    "            next_token = logits.argmax(dim=1).item()\n",
    "\n",
    "        tgt_indices.append(next_token)\n",
    "\n",
    "        # Stop at <eos>\n",
    "        if next_token == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    # Convert token indices to words\n",
    "    decoded_sentence = \" \".join([tgt_vocab.get_itos()[idx] for idx in tgt_indices[1:]])  # Skip <bos>\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TfXmVbMzVdN2"
   },
   "outputs": [],
   "source": [
    "def translate_english_to_french(sentence):\n",
    "    # Tokenize the input sentence and convert it to indices (numericalize)\n",
    "    src_tokens = numericalize(sentence, vocab_en, tokenize_english)\n",
    "    src_tensor = torch.tensor(src_tokens).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    output_tokens = model(src_tensor, src_tensor).argmax(dim=-1)\n",
    "    return \" \".join(vocab_fr.get_itos()[idx] for idx in output_tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-rXYKdKVka1",
    "outputId": "fdbf1a28-653b-4357-d8e4-e7251ebc0f14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:544: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.617, Val loss: 2.536, Epoch time = 232.695s\n",
      "Epoch: 2, Train loss: 2.465, Val loss: 1.963, Epoch time = 230.597s\n",
      "Epoch: 3, Train loss: 2.065, Val loss: 1.696, Epoch time = 232.686s\n",
      "Epoch: 4, Train loss: 1.848, Val loss: 1.537, Epoch time = 232.891s\n",
      "Epoch: 5, Train loss: 1.709, Val loss: 1.428, Epoch time = 230.976s\n",
      "Epoch: 6, Train loss: 1.607, Val loss: 1.349, Epoch time = 245.676s\n",
      "Epoch: 7, Train loss: 1.528, Val loss: 1.272, Epoch time = 245.255s\n",
      "Epoch: 8, Train loss: 1.461, Val loss: 1.217, Epoch time = 233.773s\n",
      "Epoch: 9, Train loss: 1.410, Val loss: 1.182, Epoch time = 230.468s\n",
      "Epoch: 10, Train loss: 1.394, Val loss: 1.185, Epoch time = 228.344s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Start training\n",
    "EPOCHS = 10\n",
    "train(model, translation_loader, criterion, optimizer, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DMdzYqAdfaY",
    "outputId": "fdc9abbb-86ff-4428-d4d8-a60a36461a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Sentence: how was the movie yesterday ?\n",
      "Translated French Sentence: comment était le film hier   ? <eos>\n"
     ]
    }
   ],
   "source": [
    "# English to French Conversion\n",
    "def translate_english_to_french(sentence, model, vocab_en, vocab_fr, tokenize_english):\n",
    "    return greedy_decode(model, torch.tensor(numericalize(sentence, vocab_en, tokenize_english)).unsqueeze(0).to(model.device), vocab_en, vocab_fr)\n",
    "\n",
    "# Example French sentence\n",
    "sentence_en = \"how was the movie yesterday ?\"\n",
    "\n",
    "# Translate French to English using the trained model\n",
    "translated_sentence = translate_english_to_french(sentence_en, model, vocab_en, vocab_fr, tokenize_english)\n",
    "\n",
    "# Print the translated sentence\n",
    "print(\"Original English Sentence:\", sentence_en)\n",
    "print(\"Translated French Sentence:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVWyYDVumMhz",
    "outputId": "8c6979ce-b7f5-4ac3-8cc9-0b6381ef51a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English Sentence: She loves chocolate.\n",
      "Translated French Sentence: elle adore le chocolat . <eos>\n"
     ]
    }
   ],
   "source": [
    "# English to French Conversion\n",
    "def translate_english_to_french(sentence, model, vocab_en, vocab_fr, tokenize_english):\n",
    "    return greedy_decode(model, torch.tensor(numericalize(sentence, vocab_en, tokenize_english)).unsqueeze(0).to(model.device), vocab_en, vocab_fr)\n",
    "\n",
    "# Example French sentence\n",
    "sentence_en = \"She loves chocolate.\"\n",
    "\n",
    "# Translate French to English using the trained model\n",
    "translated_sentence = translate_english_to_french(sentence_en, model, vocab_en, vocab_fr, tokenize_english)\n",
    "\n",
    "# Print the translated sentence\n",
    "print(\"Original English Sentence:\", sentence_en)\n",
    "print(\"Translated French Sentence:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky1K7figD6kF"
   },
   "source": [
    "**French to English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uutiIC0tD-SJ",
    "outputId": "f0ae4e2e-1fa3-4fe6-a482-d1e4cfe80da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (input_embedding): Embedding(23814, 256)\n",
      "  (output_embedding): Embedding(13757, 256)\n",
      "  (position_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Linear(in_features=256, out_features=13757, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Initialize model\n",
    "\n",
    "INPUT_DIM = len(vocab_fr)  # Source is French\n",
    "OUTPUT_DIM = len(vocab_en)  # Target is English\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "model_french_en = TransformerModel(\n",
    "    INPUT_DIM,\n",
    "    OUTPUT_DIM,\n",
    "    model_dim=EMB_SIZE,  # Corrected from d_model to model_dim\n",
    "    num_heads=NHEAD,  # Corrected from nhead to num_heads\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,  # Number of encoder layers\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,  # Number of decoder layers\n",
    "    dim_feedforward=FFN_HID_DIM,  # Feedforward layer dimension\n",
    "    dropout=DROPOUT  # Dropout for regularization\n",
    ").to(device)\n",
    "print(model_french_en)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_en[\"<pad>\"])\n",
    "optimizer = optim.Adam(model_french_en.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "H9WewKI9GN1t"
   },
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "BATCH_SIZE = 64\n",
    "translation_dataset = TranslationDataset(french_sentences, english_sentences, vocab_fr, vocab_en, tokenize_french, tokenize_english)\n",
    "translation_loader = DataLoader(translation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-UorPun1EXyp"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, translation_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for src, tgt in translation_loader:\n",
    "        optimizer.zero_grad()\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        output = model(src, tgt_input)  # Teacher forcing, pass source and target (except last token)\n",
    "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))  # Compare with target (except first token)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(translation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Jf9N-599FnWV"
   },
   "outputs": [],
   "source": [
    "def train(model, translation_loader, criterion, optimizer, epochs=3):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train_epoch(model, translation_loader, criterion, optimizer)\n",
    "        val_loss = evaluate(model, translation_loader, criterion)\n",
    "        print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(time.time() - epoch_start_time):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fCGDntJOFp9f"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, translation_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in translation_loader:\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            output = model(src, tgt_input)\n",
    "            loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(translation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8teidcGFtDG",
    "outputId": "de81e370-9f3b-45ce-f70a-58eac1c0eda0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.239, Val loss: 2.422, Epoch time = 143.425s\n",
      "Epoch: 2, Train loss: 2.338, Val loss: 1.825, Epoch time = 136.818s\n",
      "Epoch: 3, Train loss: 1.901, Val loss: 1.506, Epoch time = 134.293s\n",
      "Epoch: 4, Train loss: 1.640, Val loss: 1.307, Epoch time = 134.268s\n",
      "Epoch: 5, Train loss: 1.463, Val loss: 1.164, Epoch time = 134.344s\n",
      "Epoch: 6, Train loss: 1.334, Val loss: 1.057, Epoch time = 135.002s\n",
      "Epoch: 7, Train loss: 1.235, Val loss: 0.978, Epoch time = 139.765s\n",
      "Epoch: 8, Train loss: 1.156, Val loss: 0.909, Epoch time = 138.565s\n",
      "Epoch: 9, Train loss: 1.090, Val loss: 0.852, Epoch time = 137.031s\n",
      "Epoch: 10, Train loss: 1.034, Val loss: 0.802, Epoch time = 136.420s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "EPOCHS = 10\n",
    "# Train the model for French to English\n",
    "train(model_french_en, translation_loader, criterion, optimizer, epochs=EPOCHS)\n",
    "\n",
    "\n",
    "def greedy_decode_fr_to_en(model, src_tensor, src_vocab, tgt_vocab, max_len=50):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Encode the French source sentence\n",
    "    with torch.no_grad():\n",
    "        memory = model.encode(src_tensor)\n",
    "\n",
    "    # Initialize the target sentence with <bos>\n",
    "    tgt_indices = [tgt_vocab[\"<bos>\"]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # Convert the target sequence into a tensor\n",
    "        tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(model.device)  # (1, seq_len)\n",
    "\n",
    "        # Decode using current target sequence\n",
    "        with torch.no_grad():\n",
    "            output = model.decode(tgt_tensor, memory)\n",
    "            logits = model.final_layer(output[:, -1, :])  # Get logits for the last token\n",
    "            next_token = logits.argmax(dim=1).item()  # Get the highest probability token\n",
    "\n",
    "        tgt_indices.append(next_token)\n",
    "\n",
    "        # Stop decoding if <eos> token is generated\n",
    "        if next_token == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    # Convert token indices to words\n",
    "    decoded_sentence = \" \".join([tgt_vocab.get_itos()[idx] for idx in tgt_indices[1:]])  # Skip <bos>\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def translate_french_to_english(sentence, model, vocab_fr, vocab_en, tokenize_fr, max_len=50):\n",
    "\n",
    "    # Tokenize and numericalize the input French sentence\n",
    "    src_tokens = numericalize(sentence, vocab_fr, tokenize_fr)\n",
    "    src_tensor = torch.tensor(src_tokens).unsqueeze(0).to(model.device)  # (1, seq_len)\n",
    "\n",
    "    # Use greedy decoding to generate the English translation\n",
    "    return greedy_decode_fr_to_en(model, src_tensor, vocab_fr, vocab_en, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEB_V1G2MiJF",
    "outputId": "4e6ce646-f812-4ece-dd7d-5935843beceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original French Sentence: comment était le film hier   ?\n",
      "Translated English Sentence: how was the movie yesterday ? <eos>\n"
     ]
    }
   ],
   "source": [
    "# Example French sentence\n",
    "sentence_fr = \"comment était le film hier   ?\"\n",
    "\n",
    "# Translate French to English using the trained model\n",
    "translated_sentence = translate_french_to_english(sentence_fr, model_french_en, vocab_fr, vocab_en, tokenize_french)\n",
    "\n",
    "# Print the translated sentence\n",
    "print(\"Original French Sentence:\", sentence_fr)\n",
    "print(\"Translated English Sentence:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BWyf15jQeTt",
    "outputId": "c3e86a3c-5644-4332-bca2-ce3aefb7f096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original French Sentence: elle adore le chocolat .\n",
      "Translated English Sentence: she loves chocolate . <eos>\n"
     ]
    }
   ],
   "source": [
    "# Example French sentence\n",
    "sentence_fr = \"elle adore le chocolat .\"\n",
    "\n",
    "# Translate French to English using the trained model\n",
    "translated_sentence = translate_french_to_english(sentence_fr, model_french_en, vocab_fr, vocab_en, tokenize_french)\n",
    "\n",
    "# Print the translated sentence\n",
    "print(\"Original French Sentence:\", sentence_fr)\n",
    "print(\"Translated English Sentence:\", translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrCXybt6MfUh"
   },
   "source": [
    "**Comparison of English-to-French and French-to-English Translation Models**\n",
    "\n",
    "Two separate sequence-to-sequence (Seq2Seq) models were built:\n",
    "\n",
    "**English-to-French Model** – Translates sentences from English to French.\n",
    "**French-to-English Model** – Translates sentences from French to English.\n",
    "\n",
    "To evaluate performance, the output of the English-to-French model was used as input for the French-to-English model. The objective was to determine whether the back-translated sentence matched the original English sentence.\n",
    "\n",
    "**Test Case 1**\n",
    "\n",
    "**Original Sentence (English):** How was the movie yesterday?\n",
    "\n",
    "**Translated Sentence (French):** Comment était le film hier ?\n",
    "\n",
    "**Back-Translated Sentence (English):** How was the movie yesterday?\n",
    "\n",
    "The French-to-English model successfully reproduced the original sentence without any loss of meaning.\n",
    "\n",
    "**Test Case 2**\n",
    "\n",
    "**Original Sentence (English):** She loves chocolate.\n",
    "\n",
    "**Translated Sentence (French):** Elle adore le chocolat.\n",
    "\n",
    "**Back-Translated Sentence (English):** She loves chocolate.\n",
    "\n",
    "Both test cases demonstrate that the translations remain accurate and consistent when converting from English to French and back to English.\n",
    "\n",
    "**Analysis of Model Performance**\n",
    "The results indicate that both models effectively learn the mapping between English and French, preserving meaning and grammatical structure. The ability to reconstruct the original sentence suggests that the models have achieved a high level of translation quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2Dy0_d6MkG-",
    "outputId": "50368ba8-5537-452d-b6dc-36cf0d80d234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mxymbXcNedu",
    "outputId": "2c4235d8-6805-4d6b-b9e5-17568ab894d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score for Sentence 1: 1.0000\n",
      "BLEU Score for Sentence 2: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Reference translations (actual translations)\n",
    "reference_1 = [\"comment était le film hier ?\".split()]\n",
    "reference_2 = [\"elle adore le chocolat .\".split()]\n",
    "\n",
    "# Candidate translations (model-generated translations)\n",
    "candidate_1 = \"comment était le film hier ?\".split()\n",
    "candidate_2 = \"elle adore le chocolat .\".split()\n",
    "\n",
    "# Compute BLEU scores\n",
    "bleu_1 = sentence_bleu(reference_1, candidate_1)\n",
    "bleu_2 = sentence_bleu(reference_2, candidate_2)\n",
    "\n",
    "print(f\"BLEU Score for Sentence 1: {bleu_1:.4f}\")\n",
    "print(f\"BLEU Score for Sentence 2: {bleu_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCbsf4JKNhJS",
    "outputId": "49cd4cba-9f4e-4a15-9e7c-bd62ca5d358d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  Score(precision=0.8, recall=0.8, fmeasure=0.8000000000000002)\n",
      "ROUGE-2:  Score(precision=0.5, recall=0.5, fmeasure=0.5)\n",
      "ROUGE-L:  Score(precision=0.8, recall=0.8, fmeasure=0.8000000000000002)\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Define the reference and candidate translations\n",
    "reference = \"how was the movie yesterday\"\n",
    "candidate = \"how was the film yesterday\"\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Compute the ROUGE scores\n",
    "scores = scorer.score(reference, candidate)\n",
    "\n",
    "# Print the ROUGE scores\n",
    "print(\"ROUGE-1: \", scores['rouge1'])\n",
    "print(\"ROUGE-2: \", scores['rouge2'])\n",
    "print(\"ROUGE-L: \", scores['rougeL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GML9mEFOQXS",
    "outputId": "8da537f9-6232-4fc9-f781-7d94d6857b28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=a603aa7aa38bfe821a84772321b679c3c9e5dde64353208bf5386778db902b83\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3Q_22nsOlXa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
